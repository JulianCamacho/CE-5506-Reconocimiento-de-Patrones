{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales\n",
    "\n",
    "José Julián Camacho Hernández\n",
    "\n",
    "Leonardo Guillén Fernández"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 13:30:36.332141: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Cargar el set de datos MNIST\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range of 0 and 1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Seleccionar dato random del dataset\n",
    "index = np.random.randint(0, len(X_train))\n",
    "image = X_train[index]\n",
    "\n",
    "# Visualizar la imagen\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f\"Label: {y_train[index]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Feature Engineering ===#\n",
    "\n",
    "# Reshape para pasarlas a 1D\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# Estandarización de los datos\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaled_data = scaler.transform(X_train)\n",
    "X_train = scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron_Multicapa:\n",
    "    def __init__(self, capas, alpha=0.1):\n",
    "        self.capas = capas\n",
    "        self.alpha = alpha\n",
    "        self.bias = []\n",
    "        self.pesos = []\n",
    "        for i in range(0, len(capas) - 1):\n",
    "            # Inicializar los pesos y bias de cada capa\n",
    "            peso = np.random.randn(capas[i], capas[i+1])\n",
    "            self.pesos.append(peso)\n",
    "            bias = np.random.randn(capas[i+1])\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def activacion(self, x):\n",
    "        # Función de activación sigmoide\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def activacion_derivada(self, x):\n",
    "        # Derivada de la función de activación sigmoide\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        # Calcular la salida de cada capa\n",
    "        capa_activacion = [X]\n",
    "        for i in range(0, len(self.capas) - 1):\n",
    "            x = np.dot(capa_activacion[i], self.pesos[i]) + self.bias[i]\n",
    "            y = self.activacion(x)\n",
    "            capa_activacion.append(y)\n",
    "        return capa_activacion\n",
    "\n",
    "    def backpropagation(self, X, y, capa_activacion):\n",
    "        # Calcular el error de la capa de salida\n",
    "        error = capa_activacion[-1] - y\n",
    "        delta = error * self.activacion_derivada(capa_activacion[-1])\n",
    "\n",
    "        # Propagar el error hacia atrás a través de la red neuronal\n",
    "        for i in reversed(range(0, len(self.capas) - 1)):\n",
    "            activacion_actual = capa_activacion[i]\n",
    "            activacion_anterior = capa_activacion[i-1] if i > 0 else X\n",
    "            d_peso = np.outer(activacion_actual, delta)\n",
    "            d_bias = delta\n",
    "            self.pesos[i] -= self.alpha * d_peso\n",
    "            self.bias[i] -= self.alpha * d_bias\n",
    "            delta = np.dot(delta, self.pesos[i].T) * self.activacion_derivada(activacion_actual)\n",
    "\n",
    "    def entrenar(self, X, y, epochs):\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in range(0, len(X)):\n",
    "                # Feedforward\n",
    "                capa_activacion = self.feedforward(X[i])\n",
    "\n",
    "                # Backpropagation\n",
    "                self.backpropagation(X[i], y[i], capa_activacion)\n",
    "\n",
    "    def predecir(self, X):\n",
    "        # Obtener la salida de la última capa\n",
    "        capa_activacion = self.feedforward(X)\n",
    "        return capa_activacion[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMetrics(y_test, y_pred, training_time):\n",
    "    acc = accuracy_score(y_test, y_pred)                        # Calcular la exactitud\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')              # Calcular F1 score\n",
    "    rec = recall_score(y_test, y_pred, average='macro')         # Calcular el recall\n",
    "    prec = precision_score(y_test, y_pred, average='macro')     # Calcular la precisión\n",
    "    metrics = {\"Accuracy\":acc, \"Precision\":prec, \"Recall\":rec, \"F1 Score\":f1, \"Tiempo de entrenamiento [s]\":training_time}\n",
    "    df = pd.DataFrame(metrics, index = [0])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_154142/3650226470.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m#Tomar tiempo de entrenamiento\u001b[39;00m\n\u001b[1;32m      5\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> 6\u001b[0m perceptron\u001b[39m.\u001b[39;49mentrenar(X_train, np\u001b[39m.\u001b[39;49meye(\u001b[39m10\u001b[39;49m)[y_train], epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m time_taken \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[5], line 53\u001b[0m, in \u001b[0;36mPerceptron_Multicapa.entrenar\u001b[0;34m(self, X, y, epochs)\u001b[0m\n\u001b[1;32m     50\u001b[0m capa_activacion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeedforward(X[i])\n\u001b[1;32m     52\u001b[0m \u001b[39m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackpropagation(X[i], y[i], capa_activacion)\n",
      "Cell \u001b[0;32mIn[5], line 44\u001b[0m, in \u001b[0;36mPerceptron_Multicapa.backpropagation\u001b[0;34m(self, X, y, capa_activacion)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpesos[i] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m d_peso\n\u001b[1;32m     43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias[i] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m*\u001b[39m d_bias\n\u001b[0;32m---> 44\u001b[0m delta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(delta, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpesos[i]\u001b[39m.\u001b[39;49mT) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivacion_derivada(activacion_actual)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el perceptrón multicapa\n",
    "perceptron = Perceptron_Multicapa(capas=[784, 128, 10], alpha=0.15)\n",
    "\n",
    "#Tomar tiempo de entrenamiento\n",
    "start_time = time.time()\n",
    "perceptron.entrenar(X_train, np.eye(10)[y_train], epochs=15)\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de prueba\n",
    "predicciones = []\n",
    "for i in range(len(X_test)):\n",
    "    prediccion = perceptron.predecir(X_test[i])\n",
    "    prediccion_clase = np.argmax(prediccion)\n",
    "    predicciones.append(prediccion_clase)\n",
    "\n",
    "# Calcular la precisión de las predicciones\n",
    "computeMetrics(y_test, predicciones, time_taken)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Parte"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron_Multicapa_Multifuncion:\n",
    "    def __init__(self, capas, funciones, alpha=0.1):\n",
    "        self.capas = capas\n",
    "        self.funciones = funciones\n",
    "        self.alpha = alpha\n",
    "        self.bias = []\n",
    "        self.pesos = []\n",
    "        self.activation_function_dict = {\n",
    "            'sigmoid': self.activacion_sigmoid,\n",
    "            'tanh': self.activacion_tanh,\n",
    "            'relu': self.activacion_relu\n",
    "        }\n",
    "        self.derivate_function_dict = {\n",
    "            'sigmoid': self.activacion_derivada_sigmoid,\n",
    "            'tanh': self.activacion_derivada_tanh,\n",
    "            'relu': self.activacion_derivada_relu\n",
    "        }\n",
    "        for i in range(0, len(capas) - 1):\n",
    "            # Inicializar los pesos y bias de cada capa\n",
    "            peso = np.random.randn(capas[i], capas[i+1])\n",
    "            self.pesos.append(peso)\n",
    "            bias = np.random.randn(capas[i+1])\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def activacion_sigmoid(self, x):\n",
    "        # Función de activación sigmoide\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def activacion_derivada_sigmoid(self, x):\n",
    "        # Derivada de la función de activación sigmoide\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def activacion_tanh(self, x):\n",
    "        # Función de activación tanh\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def activacion_derivada_tanh(self, x):\n",
    "        # Derivada de la función de activación tanh\n",
    "        return (1 - x**2)\n",
    "    \n",
    "    def activacion_relu(self, x):\n",
    "        # Función de activación ReLU\n",
    "        return np.maximum(0.1*x, x)\n",
    "        \n",
    "    def activacion_derivada_relu(self, x):\n",
    "        # Derivada de la función de activación ReLU\n",
    "        return np.where(x > 0, 1, 0.1)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        # Calcular la salida de cada capa\n",
    "        capa_activacion = [X]\n",
    "        for i in range(0, len(self.capas) - 1):\n",
    "            #print('capa_activacion[i]', capa_activacion[i].shape)\n",
    "            #print('self.pesos[i]', self.pesos[i].shape)\n",
    "            x = np.dot(capa_activacion[i], self.pesos[i]) + self.bias[i]\n",
    "            y = self.activation_function_dict.get(self.funciones[i], lambda: None)(x)\n",
    "            capa_activacion.append(y)\n",
    "        return capa_activacion\n",
    "\n",
    "    def backpropagation(self, X, y, capa_activacion):\n",
    "        # Calcular el error de la capa de salida\n",
    "        error = capa_activacion[-1] - y\n",
    "        delta = error * self.derivate_function_dict.get(self.funciones[-1], lambda: None)(capa_activacion[-1])\n",
    "\n",
    "        # Propagar el error hacia atrás a través de la red neuronal\n",
    "        for i in reversed(range(0, len(self.capas) - 1)):\n",
    "            activacion_actual = capa_activacion[i]\n",
    "            activacion_anterior = capa_activacion[i-1] if i > 0 else X\n",
    "            d_peso = np.outer(activacion_actual, delta)\n",
    "            d_bias = delta\n",
    "            self.pesos[i] -= self.alpha * d_peso\n",
    "            self.bias[i] -= self.alpha * d_bias\n",
    "            delta = np.dot(delta, self.pesos[i].T) * self.derivate_function_dict.get(self.funciones[i], lambda: None)(activacion_actual)\n",
    "\n",
    "    def entrenar(self, X, y, epochs):\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in range(0, len(X)):\n",
    "                # Feedforward\n",
    "                capa_activacion = self.feedforward(X[i])\n",
    "\n",
    "                # Backpropagation\n",
    "                self.backpropagation(X[i], y[i], capa_activacion)\n",
    "\n",
    "    def predecir(self, X):\n",
    "        # Obtener la salida de la última capa\n",
    "        capa_activacion = self.feedforward(X)\n",
    "        return capa_activacion[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_154142/3816873217.py:27: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Tiempo de entrenamiento [s]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6622</td>\n",
       "      <td>0.7284</td>\n",
       "      <td>0.668419</td>\n",
       "      <td>0.667468</td>\n",
       "      <td>27.573038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  Tiempo de entrenamiento [s]\n",
       "0    0.6622     0.7284  0.668419  0.667468                    27.573038"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def MLP_Functions(capas, funciones, alpha, epochs):\n",
    "\n",
    "    # Crear y entrenar el perceptrón multicapa\n",
    "    perceptron = Perceptron_Multicapa_Multifuncion(capas=capas, funciones=funciones, alpha=alpha)\n",
    "\n",
    "    #Tomar tiempo de entrenamiento\n",
    "    start_time = time.time()\n",
    "    perceptron.entrenar(X_train, np.eye(10)[y_train], epochs=epochs)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # Hacer predicciones sobre el conjunto de prueba\n",
    "    predicciones = []\n",
    "    for i in range(len(X_test)):\n",
    "        prediccion = perceptron.predecir(X_test[i])\n",
    "        prediccion_clase = np.argmax(prediccion)\n",
    "        predicciones.append(prediccion_clase)\n",
    "\n",
    "    # Calcular la precisión de las predicciones\n",
    "    computeMetrics(y_test, predicciones, time_taken)\n",
    "\n",
    "MLP_Functions([784, 8, 5, 10],['sigmoid','sigmoid', 'sigmoid'] , 0.15, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1680000x28 and 784x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m     27\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m     outputs \u001b[39m=\u001b[39m model(X_train_tensor)\n\u001b[1;32m     30\u001b[0m     y_train_tensor_long \u001b[39m=\u001b[39m y_train_tensor\u001b[39m.\u001b[39mlong()\n\u001b[1;32m     31\u001b[0m     y_train_tensor_onehot \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(y_train_tensor_long, num_classes\u001b[39m=\u001b[39moutput_size)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[105], line 10\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[1;32m     11\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[1;32m     12\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1680000x28 and 784x128)"
     ]
    }
   ],
   "source": [
    "# Define the MLP architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)  # Second hidden layer with a different activation function\n",
    "        self.fc3 = nn.Linear(64, output_size)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the MLP model\n",
    "model = MLP(784, 10)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "\n",
    "    y_train_tensor_long = y_train_tensor.long()\n",
    "    y_train_tensor_onehot = F.one_hot(y_train_tensor_long, num_classes=output_size).float()\n",
    "\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "outputs = model(X_test)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "accuracy = (predicted == y_test).sum().item() / len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Cargar los datos de ejemplo (Iris dataset)\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_entrenamiento, X_prueba, y_entrenamiento, y_prueba = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir los datos a tensores de PyTorch\n",
    "X_entrenamiento = torch.Tensor(X_entrenamiento)\n",
    "X_prueba = torch.Tensor(X_prueba)\n",
    "y_entrenamiento = torch.Tensor(y_entrenamiento).long()\n",
    "y_prueba = torch.Tensor(y_prueba).long()\n",
    "\n",
    "# Definir la arquitectura de la red neuronal\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 100)  # Capa oculta 1\n",
    "        self.fc2 = nn.Linear(100, 100)  # Capa oculta 2\n",
    "        self.fc3 = nn.Linear(100, 3)  # Capa de salida\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Función de activación ReLU en la capa oculta 1\n",
    "        x = torch.tanh(self.fc2(x))  # Función de activación tanh en la capa oculta 2\n",
    "        x = self.fc3(x)  # Capa de salida sin función de activación\n",
    "        return x\n",
    "\n",
    "# Crear el modelo de la red neuronal\n",
    "modelo = Net()\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterio = nn.CrossEntropyLoss()\n",
    "optimizador = optim.SGD(modelo.parameters(), lr=0.01)\n",
    "\n",
    "# Entrenar la red neuronal\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Paso de entrenamiento\n",
    "    optimizador.zero_grad()\n",
    "    salida = modelo(X_entrenamiento)\n",
    "    perdida = criterio(salida, y_entrenamiento)\n",
    "    perdida.backward()\n",
    "    optimizador.step()\n",
    "\n",
    "# Hacer predicciones sobre los datos de prueba\n",
    "with torch.no_grad():\n",
    "    salida = modelo(X_prueba)\n",
    "    _, predicciones = torch.max(salida, 1)\n",
    "\n",
    "# Calcular la precisión de las predicciones\n",
    "precision = accuracy_score(y_prueba, predicciones)\n",
    "\n",
    "print(\"Precisión:\", precision)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

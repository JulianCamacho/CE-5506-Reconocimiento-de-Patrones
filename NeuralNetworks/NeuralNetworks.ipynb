{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales\n",
    "\n",
    "José Julián Camacho Hernández\n",
    "\n",
    "Leonardo Guillén Fernández"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I Parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Cargar el set de datos MNIST\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range of 0 and 1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Seleccionar dato random del dataset\n",
    "index = np.random.randint(0, len(X_train))\n",
    "image = X_train[index]\n",
    "\n",
    "# Visualizar la imagen\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f\"Label: {y_train[index]}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=== Feature Engineering ===#\n",
    "\n",
    "# Reshape para pasarlas a 1D\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# Estandarización de los datos\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaled_data = scaler.transform(X_train)\n",
    "X_train = scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron_Multicapa:\n",
    "    def __init__(self, capas, alpha=0.1):\n",
    "        self.capas = capas\n",
    "        self.alpha = alpha\n",
    "        self.bias = []\n",
    "        self.pesos = []\n",
    "        for i in range(0, len(capas) - 1):\n",
    "            # Inicializar los pesos y bias de cada capa\n",
    "            peso = np.random.randn(capas[i], capas[i+1])\n",
    "            self.pesos.append(peso)\n",
    "            bias = np.random.randn(capas[i+1])\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def activacion(self, x):\n",
    "        # Función de activación sigmoide\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def activacion_derivada(self, x):\n",
    "        # Derivada de la función de activación sigmoide\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        # Calcular la salida de cada capa\n",
    "        capa_activacion = [X]\n",
    "        for i in range(0, len(self.capas) - 1):\n",
    "            x = np.dot(capa_activacion[i], self.pesos[i]) + self.bias[i]\n",
    "            y = self.activacion(x)\n",
    "            capa_activacion.append(y)\n",
    "        return capa_activacion\n",
    "\n",
    "    def backpropagation(self, X, y, capa_activacion):\n",
    "        # Calcular el error de la capa de salida\n",
    "        error = capa_activacion[-1] - y\n",
    "        delta = error * self.activacion_derivada(capa_activacion[-1])\n",
    "\n",
    "        # Propagar el error hacia atrás a través de la red neuronal\n",
    "        for i in reversed(range(0, len(self.capas) - 1)):\n",
    "            activacion_actual = capa_activacion[i]\n",
    "            activacion_anterior = capa_activacion[i-1] if i > 0 else X\n",
    "            d_peso = np.outer(activacion_actual, delta)\n",
    "            d_bias = delta\n",
    "            self.pesos[i] -= self.alpha * d_peso\n",
    "            self.bias[i] -= self.alpha * d_bias\n",
    "            delta = np.dot(delta, self.pesos[i].T) * self.activacion_derivada(activacion_actual)\n",
    "\n",
    "    def entrenar(self, X, y, epochs):\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in range(0, len(X)):\n",
    "                # Feedforward\n",
    "                capa_activacion = self.feedforward(X[i])\n",
    "\n",
    "                # Backpropagation\n",
    "                self.backpropagation(X[i], y[i], capa_activacion)\n",
    "\n",
    "    def predecir(self, X):\n",
    "        # Obtener la salida de la última capa\n",
    "        capa_activacion = self.feedforward(X)\n",
    "        return capa_activacion[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMetrics(y_test, y_pred, training_time):\n",
    "    acc = accuracy_score(y_test, y_pred)                        # Calcular la exactitud\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')              # Calcular F1 score\n",
    "    rec = recall_score(y_test, y_pred, average='macro')         # Calcular el recall\n",
    "    prec = precision_score(y_test, y_pred, average='macro')     # Calcular la precisión\n",
    "    metrics = {\"Accuracy\":acc, \"Precision\":prec, \"Recall\":rec, \"F1 Score\":f1, \"Tiempo de entrenamiento [s]\":training_time}\n",
    "    df = pd.DataFrame(metrics, index = [0])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8283/3650226470.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-x))\n",
      "/home/jose/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Tiempo de entrenamiento [s]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7432</td>\n",
       "      <td>0.765724</td>\n",
       "      <td>0.752851</td>\n",
       "      <td>0.734011</td>\n",
       "      <td>641.11168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  Tiempo de entrenamiento [s]\n",
       "0    0.7432   0.765724  0.752851  0.734011                    641.11168"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear y entrenar el perceptrón multicapa\n",
    "perceptron = Perceptron_Multicapa(capas=[784, 128, 10], alpha=0.15)\n",
    "\n",
    "#Tomar tiempo de entrenamiento\n",
    "start_time = time.time()\n",
    "perceptron.entrenar(X_train, np.eye(10)[y_train], epochs=15)\n",
    "end_time = time.time()\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Hacer predicciones sobre el conjunto de prueba\n",
    "predicciones = []\n",
    "for i in range(len(X_test)):\n",
    "    prediccion = perceptron.predecir(X_test[i])\n",
    "    prediccion_clase = np.argmax(prediccion)\n",
    "    predicciones.append(prediccion_clase)\n",
    "\n",
    "# Calcular la precisión de las predicciones\n",
    "computeMetrics(y_test, predicciones, time_taken)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II Parte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron_Multicapa_Multifuncion:\n",
    "    def __init__(self, capas, funciones, alpha=0.1):\n",
    "        self.capas = capas\n",
    "        self.funciones = funciones\n",
    "        self.alpha = alpha\n",
    "        self.bias = []\n",
    "        self.pesos = []\n",
    "        self.activation_function_dict = {\n",
    "            'sigmoid': self.activacion_sigmoid,\n",
    "            'tanh': self.activacion_tanh,\n",
    "            'relu': self.activacion_relu\n",
    "        }\n",
    "        self.derivate_function_dict = {\n",
    "            'sigmoid': self.activacion_derivada_sigmoid,\n",
    "            'tanh': self.activacion_derivada_tanh,\n",
    "            'relu': self.activacion_derivada_relu\n",
    "        }\n",
    "        for i in range(0, len(capas) - 1):\n",
    "            # Inicializar los pesos y bias de cada capa\n",
    "            peso = np.random.randn(capas[i], capas[i+1])\n",
    "            self.pesos.append(peso)\n",
    "            bias = np.random.randn(capas[i+1])\n",
    "            self.bias.append(bias)\n",
    "\n",
    "    def activacion_sigmoid(self, x):\n",
    "        # Función de activación sigmoide\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def activacion_derivada_sigmoid(self, x):\n",
    "        # Derivada de la función de activación sigmoide\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def activacion_tanh(self, x):\n",
    "        # Función de activación tanh\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def activacion_derivada_tanh(self, x):\n",
    "        # Derivada de la función de activación tanh\n",
    "        return (1 - x**2)\n",
    "    \n",
    "    def activacion_relu(self, x):\n",
    "        # Función de activación ReLU\n",
    "        return np.maximum(0.1*x, x)\n",
    "        \n",
    "    def activacion_derivada_relu(self, x):\n",
    "        # Derivada de la función de activación ReLU\n",
    "        return np.where(x > 0, 1, 0.1)\n",
    "\n",
    "    def feedforward(self, X):\n",
    "        # Calcular la salida de cada capa\n",
    "        capa_activacion = [X]\n",
    "        for i in range(0, len(self.capas) - 1):\n",
    "            #print('capa_activacion[i]', capa_activacion[i].shape)\n",
    "            #print('self.pesos[i]', self.pesos[i].shape)\n",
    "            x = np.dot(capa_activacion[i], self.pesos[i]) + self.bias[i]\n",
    "            y = self.activation_function_dict.get(self.funciones[i], lambda: None)(x)\n",
    "            capa_activacion.append(y)\n",
    "        return capa_activacion\n",
    "\n",
    "    def backpropagation(self, X, y, capa_activacion):\n",
    "        # Calcular el error de la capa de salida\n",
    "        error = capa_activacion[-1] - y\n",
    "        delta = error * self.derivate_function_dict.get(self.funciones[-1], lambda: None)(capa_activacion[-1])\n",
    "\n",
    "        # Propagar el error hacia atrás a través de la red neuronal\n",
    "        for i in reversed(range(0, len(self.capas) - 1)):\n",
    "            activacion_actual = capa_activacion[i]\n",
    "            activacion_anterior = capa_activacion[i-1] if i > 0 else X\n",
    "            d_peso = np.outer(activacion_actual, delta)\n",
    "            d_bias = delta\n",
    "            self.pesos[i] -= self.alpha * d_peso\n",
    "            self.bias[i] -= self.alpha * d_bias\n",
    "            delta = np.dot(delta, self.pesos[i].T) * self.derivate_function_dict.get(self.funciones[i], lambda: None)(activacion_actual)\n",
    "\n",
    "    def entrenar(self, X, y, epochs):\n",
    "        for epoch in range(0, epochs):\n",
    "            for i in range(0, len(X)):\n",
    "                # Feedforward\n",
    "                capa_activacion = self.feedforward(X[i])\n",
    "\n",
    "                # Backpropagation\n",
    "                self.backpropagation(X[i], y[i], capa_activacion)\n",
    "\n",
    "    def predecir(self, X):\n",
    "        # Obtener la salida de la última capa\n",
    "        capa_activacion = self.feedforward(X)\n",
    "        return capa_activacion[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5474/3816873217.py:27: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Tiempo de entrenamiento [s]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5676</td>\n",
       "      <td>0.674138</td>\n",
       "      <td>0.565976</td>\n",
       "      <td>0.559697</td>\n",
       "      <td>21.78835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Precision    Recall  F1 Score  Tiempo de entrenamiento [s]\n",
       "0    0.5676   0.674138  0.565976  0.559697                     21.78835"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def MLP_Functions(capas, funciones, alpha, epochs):\n",
    "\n",
    "    # Crear y entrenar el perceptrón multicapa\n",
    "    perceptron = Perceptron_Multicapa_Multifuncion(capas=capas, funciones=funciones, alpha=alpha)\n",
    "\n",
    "    #Tomar tiempo de entrenamiento\n",
    "    start_time = time.time()\n",
    "    perceptron.entrenar(X_train, np.eye(10)[y_train], epochs=epochs)\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    # Hacer predicciones sobre el conjunto de prueba\n",
    "    predicciones = []\n",
    "    for i in range(len(X_test)):\n",
    "        prediccion = perceptron.predecir(X_test[i])\n",
    "        prediccion_clase = np.argmax(prediccion)\n",
    "        predicciones.append(prediccion_clase)\n",
    "\n",
    "    # Calcular la precisión de las predicciones\n",
    "    computeMetrics(y_test, predicciones, time_taken)\n",
    "\n",
    "MLP_Functions([784, 8, 5, 10],['sigmoid','sigmoid', 'sigmoid'] , 0.15, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1680000x28 and 784x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m     27\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m     outputs \u001b[39m=\u001b[39m model(X_train_tensor)\n\u001b[1;32m     30\u001b[0m     y_train_tensor_long \u001b[39m=\u001b[39m y_train_tensor\u001b[39m.\u001b[39mlong()\n\u001b[1;32m     31\u001b[0m     y_train_tensor_onehot \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(y_train_tensor_long, num_classes\u001b[39m=\u001b[39moutput_size)\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[105], line 10\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[1;32m     11\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x))\n\u001b[1;32m     12\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1680000x28 and 784x128)"
     ]
    }
   ],
   "source": [
    "# Define the MLP architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(128, 64)  # Second hidden layer with a different activation function\n",
    "        self.fc3 = nn.Linear(64, output_size)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the MLP model\n",
    "model = MLP(784, 10)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "\n",
    "    y_train_tensor_long = y_train_tensor.long()\n",
    "    y_train_tensor_onehot = F.one_hot(y_train_tensor_long, num_classes=output_size).float()\n",
    "\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "outputs = model(X_test)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "accuracy = (predicted == y_test).sum().item() / len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
